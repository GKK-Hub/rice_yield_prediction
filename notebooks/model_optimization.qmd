---
title: "Model Training and Optimization"
author: Gowtham Kumar K
date: September 25, 2025
link-external-newwindow: true
format:
  html:
    smooth-scroll: true
    encoding: utf-8
    toc: true
    toc-location: right
    toc-depth: 3
    toc-title: Chapters
    theme: zephyr
    monofont: "JetBrains Mono"
    page-layout: full
    code-tools: true
    include-in-header: 
      text: |
        <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel="stylesheet">
        <style>
          code, pre {
            font-family: 'JetBrains Mono', monospace !important;
          }
        </style>
        <meta charset="UTF-8">
execute:
  enabled: true
  echo: true
  output: true
server: shiny
jupyter: python3

---

In this file, we\'re going to do model training and optimization. Finally, we\'ll save the best performing model and use that for SHAP analysis and model deployment.


```{python}
#| code-fold: true

# Standard library imports
import random
import warnings

# Third-party library imports
import mlflow
import numpy as np
import pandas as pd
import sklearn
from shiny.express import input, render, ui
from sklearn import set_config
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

# Local/project imports
import rice_yield.models.mlflow_utils as mlf
import rice_yield.models.optuna_utils as opt
from rice_yield.models.mlflow_utils import set_run_tags, start_run
from rice_yield.utils.notebook_utils import render_table, save_model
from rice_yield.utils.paths import get_data_dir, get_data_file, get_output_dir
from rice_yield.utils.plot_utils import plot_prediction_error, plot_train_test_scores, train_test_plot, validation_curve_display
warnings.filterwarnings("ignore")
```


```{python}
#| code-fold: true
#| 
file_path = get_data_file(get_data_dir("final"), "rice-yield.csv")

df = pd.read_csv(file_path)
```

---

## Chapter 6: Split Data - Train, Valid and Test

Let\'s sort the data before splitting the data.

```{python}
df = df.sort_values(['year', 'dist_name'], ignore_index=True)
render_table(df)
```

---

We\'ll use 60% of the data for training and the remaining for testing purpose.

---

```{python}
years = sorted(df['year'].unique())
total_years = len(years)

# Calculate number of years for each split
train_years = int(total_years * 0.6) # 60 % for training
val_years = int(total_years * 0.2)  # 20 % for vaildating, remaining for testing

train_cutoff = years[train_years - 1]  # we're indexing an array so we subtract 1, otherwise will get 1 more than train/val_years
val_cutoff = years[train_years + val_years - 1]  

train_df = df[df['year'] <= train_cutoff]
val_df = df[(df['year'] > train_cutoff) & (df['year'] <= val_cutoff)]
test_df = df[df['year'] > val_cutoff]
```

::: {.panel-tabset}
### Train DataFrame

```{python}
#| code-fold: true
render_table(train_df)
```

### Valid DataFrame

```{python}
#| code-fold: true
render_table(val_df)
```

### Test DataFrame
```{python}
#| code-fold: true
render_table(test_df)
```
:::


```{python}
def assign_split(x, train_cutoff, val_cutoff):
    if x <= train_cutoff:
        return "Train"
    elif x <= val_cutoff:
        return "Val"
    else:
        return "Test"

# Creating a new column `split` just for plotting purpose and it doesn't go into training
df['split'] = df['year'].apply(lambda x: assign_split(x, train_cutoff, val_cutoff))
```

```{python}
def show_splits():
    @render.ui() # Creates a drop-down and chooses the `default_choice` by default
    def drop_down_feature():
        choices = df.columns.difference(['split', 'dist_name', 'year']).tolist()
        default_choice = choices[0] if choices else None
        user_input = ui.input_select('feature', 
                                     label='Choose Variable',
                                     choices=choices,
                                     selected=default_choice)
        return user_input

    @render.plot() # Renders the plot based on the variable chosen in the drop-down
    def plot_split():   
        fig, ax = train_test_plot(df, input.feature())
        return fig   
         
    return drop_down_feature, plot_split                    
```


```{python}
user_input, split_plot = show_splits()
user_input
split_plot
```


### Train, Valid and Test Set
```{python}
target = "yield"
X_train = train_df.drop(columns=target)
y_train = train_df[target]

X_val = val_df.drop(columns=target)
y_val = val_df[target]

X_test = test_df.drop(columns=target)
y_test = test_df[target]
```


### Time-series Split


```{python}
set_config(display="diagram")

# Since we have a `year` variable, random split wouldn't work.
tscv = TimeSeriesSplit(n_splits=5)
```


## Chapter 7: Data Preprocessing

In this chapter, we\'re using a preprocessing pipleine to transform categorical variable (`dist_name`) and numerical variables (for faster training) in our dataset. Preprocessing of categorical variables are essential as machine learning models don't work with text data natively.


```{python}
# Column Selector
numeric_selector = selector(dtype_exclude=object)
categoric_selector = selector(dtype_include=object)
```


```{python}
numeric_columns = numeric_selector(X_train)
categoric_columns = categoric_selector(X_train)
```


### Preprocessor Pipeline
```{python}
categoric_transformer = OneHotEncoder(handle_unknown="ignore")
numeric_transformer = MinMaxScaler()

preprocessor = ColumnTransformer(transformers=[('categoric', categoric_transformer, categoric_columns),
                                               ('numeric', numeric_transformer, numeric_columns)], 
                                                remainder="passthrough", 
                                                verbose_feature_names_out=False)

preprocessor
```

# Chapter 7: Validation Curves

```{python}
candidate_models = {
    "Ridge Regression": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                  ('ridge', Ridge(max_iter=1000))]),
        "param_grid": {
            "ridge__alpha": [0.05, 0.02, 0.01, 0.1, 0.2, 0.5, 1, 2]  # Regularization strength
        }
    },
    
    "Support Vector Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                  ('svr', SVR(max_iter=500))]),
        "param_grid": {
            "svr__C": [1, 5, 10, 20, 50],
            "svr__epsilon": [0.1, 1, 1.5, 2],
            "svr__gamma": [0.0001, 0.002, 0.01, 0.1, 1, 1.2, 2],
        }
    },
    
    "Random Forest Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                  ('rf', RandomForestRegressor())]),
        "param_grid": {
            "rf__n_estimators": [50, 100, 150, 200],
            "rf__max_depth": [3, 5, 8, 10, 12],
            "rf__min_samples_split": [2, 3, 5, 8, 10],
        }
    },

    "Extra Trees Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                  ('etr', ExtraTreesRegressor())]),
        "param_grid": {
            "etr__n_estimators": [1, 2, 3, 4, 5],
            "etr__max_depth": [1, 2, 3, 4],
            "etr__min_samples_split": [5, 8, 12, 15, 20],
            "etr__min_samples_leaf": [2, 3, 4, 5, 6]
        }
    },
    
    "Decision Tree Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                  ('dt', DecisionTreeRegressor())]),
        "param_grid": {
            "dt__max_depth": [1, 2, 3, 4, 5, 8],
            "dt__min_samples_split": [5, 8, 12, 15, 20],
            "dt__min_samples_leaf": [5, 10, 15, 20],
        }
    },
    
    "Ada Boost Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                  ('ada', AdaBoostRegressor(DecisionTreeRegressor()))]),
        "param_grid": {
            "ada__n_estimators": [1, 2, 5, 8, 10],
            "ada__learning_rate": [0.001, 0.01, 1, 1.2, 2, 3],
        }
    },
    
    "XGBoost Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor),
                                  ('xgb', XGBRegressor())]),
        "param_grid": {
            "xgb__n_estimators": [1, 2, 5, 8, 10],
            "xgb__learning_rate": [0.01, 0.1, 0.2],
            "xgb__max_depth": [1, 2, 3, 5]
        }
    }
}
```

```{python}
model_names = list(candidate_models.keys())
default_model = model_names[0] if model_names else None
```


```{python}
def show_validation_curves():
    @render.ui()
    def dropdown_model():
        return ui.input_select(id='model',
                                label='Select Model',
                                choices=model_names,
                                selected=default_model)
    
    @render.ui()
    def dropdown_param():
        params = list(candidate_models[input.model()]['param_grid'].keys())
        default_param = params[0] if params else None
        return ui.input_select(id='param',
                                label='Select Hyperparameter',
                                choices=params,
                                selected=default_param)
    
    @render.plot(width=900, height=500)  # type: ignore
    def show_plot():
        try:
            model_name = input.model()
            param_name = input.param()
            
            # Check if the model exists
            if model_name not in candidate_models:
                return None
            
            # Check if the parameter exists for the selected model. Because when we change the model directly,
            # the previous model's parameters will still be there briefly and would show error as the changed model
            # doesn't necessarily have previous model's parameter.
            available_params = list(candidate_models[model_name]['param_grid'].keys())
            if param_name not in available_params:
                # If the current param doesn't exist for this model, use the first available param of the chosen model
                param_name = available_params[0] if available_params else None
                

            plot_params = {
                'estimator': candidate_models[model_name]['pipeline'],
                'X': X_train,
                'y': y_train,
                'param_name': param_name,
                'param_range': candidate_models[model_name]['param_grid'][param_name],
                'cv': tscv,
                'scoring': 'neg_mean_absolute_error',
            }
            
            fig, ax = validation_curve_display(**plot_params)
            return fig
            
        except (KeyError, TypeError, ValueError) as e:
            return None
    
    return dropdown_model, dropdown_param, show_plot
```

```{python}
#| error: false
model, param, plot = show_validation_curves()
model
param
plot
```

# Chapter 8: Hyperparameter Optimization


```{python}
# Candidate models and their search spaces defined with the help of observations from Validation Curves

search_spaces = {
    "SVR": {
        "model_class": SVR,
        "params": lambda trial: {
            "C": trial.suggest_float("C", 50, 120), # 10, 100 
            "epsilon": trial.suggest_float("epsilon", 0, 5),
            "gamma": trial.suggest_float("gamma", 1e-3, 1e-1, log=True), # 1e-3, 1e1
            "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
        }
    },
    "RandomForest": {
        "model_class": RandomForestRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 10, 100), # 10, 100
            "max_depth": trial.suggest_int("max_depth", 2, 10), # 2, 10
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 20) # 2, 20
        }
    },
    "AdaBoost": {
        "model_class": AdaBoostRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 5, 15), # 1, 15
            "learning_rate": trial.suggest_float("learning_rate", 1, 6) # 0.0001, 5
        }
    },
    "XGB": {
        "model_class": XGBRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 5, 25), # 2, 20
            "max_depth": trial.suggest_int("max_depth", 3, 15), # 1, 10
            "learning_rate": trial.suggest_float("learning_rate", 1, 5) # 0.00001, 3
        }
    },
    "ExtraTrees": {
        "model_class": ExtraTreesRegressor,
        "params": lambda trial: {
            "min_samples_split": trial.suggest_int("min_samples_split", 5, 20),
            "max_depth": trial.suggest_int("max_depth", 1, 5),
            "n_estimators": trial.suggest_int("n_estimators", 1, 5),
            "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 6)
        }
    },
    "DecisionTrees": {
        "model_class": DecisionTreeRegressor,
        "params": lambda trial: {
            "min_samples_leaf": trial.suggest_int("min_samples_leaf", 3, 20),
            "max_depth": trial.suggest_int("max_depth", 1, 10),
            "min_samples_split": trial.suggest_int("min_samples_split", 5, 20)
        }
    }
}
```


### Registering Train Dataset in MLflow

```{python}
dataset_source_url = "http://data.icrisat.org/dld/src/crops.html"
dataset_source = mlf.set_source_url(url=dataset_source_url)
dataset_name = "ICRISAT DLD dataset"
dataset_target = "yield"
```


```{python}
dataset = mlf.pd_dataset(data=pd.concat([X_train, y_train], axis=1), 
                         source=dataset_source, 
                         target=dataset_target, 
                         name=dataset_name)
```


```{python}
def make_objective(model_name, model_class, param_fn):
    
    def objective(trial):
        # Suggest hyperparameters
        params = param_fn(trial)

        # Start MLflow run for this trial
        with start_run(model_name=model_name, 
                       experiment_type="HPO", 
                       run_name=f"{model_name}_trial_{trial.number}", 
                       nest=True) as run:
            trial.set_user_attr("mlflow_run_id", mlf.get_active_run_id())
            
            # Build pipeline
            model = Pipeline([('preprocessor', preprocessor), 
                              (model_name.lower(), model_class(**params))])

            mae_scores = cross_val_score(model, 
                                         X_train, 
                                         y_train, 
                                         cv=tscv, 
                                         scoring="neg_mean_absolute_error")

            rmse_scores = cross_val_score(model,
                                          X_train,
                                          y_train,
                                          cv=tscv,
                                          scoring="neg_root_mean_squared_error")

            mae = -np.mean(mae_scores)
            rmse = -np.mean(rmse_scores)


            # Optional: log hyperparameters as tags
            set_run_tags(params)
            
            # Log metrics into MLflow
            mlf.log_metrics({"mae": mae, "rmse": rmse})
            mlf.log_input(dataset, context="training")

        return mae

    return objective
```


```{python}
model_results = {}

for model_name, model_params in search_spaces.items():
    with mlf.start_run(model_name, "HPO", f"{model_name}_optimization") as parent_run:

        study = opt.create_study(study_name=f"{model_name}_Study",
                                 direction="minimize",
                                 storage="sqlite:///../outputs/models_db.sqlite3")

        print("Optimization Started for", model_name)
        objective_func = make_objective(model_name,
                                        model_params["model_class"],
                                        model_params["params"])
        study.optimize(objective_func, n_trials=10)
        print("Optimization completed for", model_name)

        best_trial = opt.get_best_trial(study)
        best_run_id = opt.get_best_run_id(study)

        mlf.set_run_tags({"best_trial_number": opt.get_best_trail_number(study)})
        mlf.log_input(dataset, context='training')
        mlf.log_params(opt.get_best_params(study))
        mlf.log_metrics({"mae": best_trial.value})


        best_model = Pipeline([('preprocessor', preprocessor),
                               (model_name.lower(), model_params['model_class'](**best_trial.params))])

        # Essential to fit the model to be used for `prediction_error_plot`
        # Just doing hyperparameter optimization, doesn't fit the model to be called predict method on
        best_model.fit(X_train, y_train)

        train_scores = cross_val_score(best_model, 
                                       X_train, 
                                       y_train, 
                                       cv=tscv, 
                                       scoring='neg_mean_absolute_error') 

        output_folder = get_output_dir() / "trained_models"
        save_model(best_model, model_name, output_folder)

        model_results[model_name] = {
                                    "model": best_model,
                                    "best_params": opt.get_best_params(study),
                                    "study": study,
                                    "cv_results": {
                                        "train_score": train_scores}
                                    }
       
    mlf.end_run()
```


```{python}
@render.ui
def dropdon_models():
    return ui.input_select(id="models",
                           label="Select Model(s)",
                           choices=sorted(model_results.keys()))

@render.plot
def prediction_error_plot():
    model_info = model_results[input.models()]  
    model = model_info["model"]
    fig, ax = plot_prediction_error(y_val, model.predict(X_val))
    fig.suptitle(f"Validation Set Error for {input.models()}", 
                 fontsize=14, 
                 fontweight='bold', 
                 fontfamily='Arial')
    return fig

```


```{python}
@render.ui
def dropdwn_models():
    return ui.input_select(id="model_scores",
                           label="Select Model(s)",
                           choices=sorted(model_results.keys()))


@render.plot
def plot_train_test():
    model_name = input.model_scores()
    cv = model_results[model_name]["cv_results"]
    val_scores = cross_val_score(best_model, 
                                 X_val, 
                                 y_val, 
                                 cv=tscv, 
                                 scoring='neg_mean_absolute_error')

    fig, ax = plot_train_test_scores(train_scores=-cv['train_score'],
                                     test_scores=-val_scores,
                                     model_name=model_name,
                                     scoring_name="MAE")
    return fig
```


## Training on  Train + Val Data before Deployment
```{python}
X_train = pd.concat([X_train, X_val], axis=0)
y_train = pd.concat([y_train, y_val], axis=0)

best_model = model_results['RandomForest']['model']
best_model.fit(X_train, y_train)
```

```{python}
data_dir = get_data_dir("final")
save_model(best_model, 'rf', get_output_dir()/"best_model")

X_train.to_csv(data_dir/"X_train.csv", index=False)
X_test.to_csv(data_dir/"X_test.csv", index=False)
X_val.to_csv(data_dir/"X_val.csv", index=False)
y_train.to_csv(data_dir/"y_train.csv", index=False)
y_val.to_csv(data_dir/"y_val.csv", index=False)
y_test.to_csv(data_dir/"y_test.csv", index=False)
```
