---
title: "Model Training and Optimization"
format:
  html:
    smooth-scroll: true
    encoding: utf-8
    toc: true
    toc-location: right
    toc-depth: 3
    toc-title: Chapters
    theme: zephyr
    # monofont: "JetBrains Mono"
    page-layout: full
    code-tools: true
    include-in-header: 
      text: |
        <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel="stylesheet">
        <style>
          code, pre {
            font-family: 'JetBrains Mono', monospace !important;
          }
        </style>
        <meta charset="UTF-8">
execute:
  enabled: true
  echo: true
  output: true
server: shiny
jupyter: python3

---

In this file, we\'re going to do model training and optimization. Finally, we\'ll save the best performing model and use that for inference and SHAP analysis.


```{python}
#| code-fold: true
import random
import warnings
import pandas as pd
import numpy as np

warnings.filterwarnings("ignore")
```


```{python}
#| code-fold: true
from rice_yield.utils.paths import get_data_file, get_data_dir
from rice_yield.utils.notebook_utils import render_table

file_path = get_data_file(get_data_dir("final"), "yield_processed.csv")

df = pd.read_csv(file_path)
```

---

## Chapter 6: Split Data - Train, Valid and Test

Let\'s sort the data before splitting the data.

```{python}
df = df.sort_values(['year', 'dist_name'], ignore_index=True)
render_table(df)
```

---

We\'ll use 80% of the data for training and the remaining for testing purpose.

---

```{python}
cutoff = int(df['year'].nunique() * 0.8) + df['year'].min() 
```


```{python}
train_df = df[df['year'] <= cutoff].copy()
test_df = df[df['year'] > cutoff].copy()
```

::: {.panel-tabset}
### Train DataFrame

```{python}
#| code-fold: true
render_table(train_df)
```

### Test DataFrame
```{python}
#| code-fold: true

render_table(test_df)
```
:::


```{python}
#| code-fold: true
df['split'] = df['year'].apply(lambda x: "Train" if x <= cutoff else "Test")
```


```{python}
from rice_yield.utils.plot_utils import show_splits

user_input, split_plot = show_splits(df)
user_input
split_plot
```

### Train and Test Set
```{python}
target = "yield"
X_train = train_df.drop(columns=target)
y_train = train_df[target]

X_test = test_df.drop(columns=target)
y_test = test_df[target]
```


### Time-series Split


```{python}
from sklearn import set_config
from sklearn.model_selection import TimeSeriesSplit

set_config(display="diagram")

tscv = TimeSeriesSplit(n_splits=5)
```

```{python}
#| code-fold: true
# for i, (train_idx, test_idx) in enumerate(tscv.split(X_train)):
#   print(f"Split {i + 1}: Train size = {len(train_idx)}, Test size = {len(test_idx)}")
```

## Chapter 7: Data Preprocessing

In this chapter, we\'re using a preprocessing pipleine to transform categoric data (`dist_name`) in our dataset. Also, some models like Linear Regression benefit from scaling numerical data.


```{python}
# Column Selector

from sklearn.compose import make_column_selector as selector

numeric_selector = selector(dtype_exclude=object)
categoric_selector = selector(dtype_include=object)
```


```{python}
numeric_columns = numeric_selector(X_train)
categoric_columns = categoric_selector(X_train)
```


### Preprocessor Pipeline
```{python}
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

categoric_transformer = OneHotEncoder(handle_unknown="ignore")
numeric_transformer = StandardScaler()

preprocessor = ColumnTransformer(transformers=[('categoric', categoric_transformer, categoric_columns),
('numeric', numeric_transformer, numeric_columns)], remainder="passthrough", verbose_feature_names_out=False)

preprocessor
```

# Chapter 7: Validation Curves

```{python}
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor
from xgboost import XGBRegressor

from sklearn.pipeline import Pipeline
```

```{python}
candidate_models = {
  "Ridge Regression": {
    "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('ridge', Ridge(max_iter=1000))]),
    "param_grid": {
        "ridge__alpha": [0.1, 1.0, 10.0, 100.0]  # Regularization strength
    }
},
    "Support Vector Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(max_iter=500))]),
        "param_grid": {
            "svr__C": [800, 1000, 1100],
            "svr__epsilon": [2, 2.5, 3],
            "svr__gamma": [0.00001, 0.0001, 0.001],
        }
    },

    "Extra Trees Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('etr', ExtraTreesRegressor())]),
        "param_grid": {
            "etr__min_samples_split": [2, 3, 5, 10, 15, 20],
            "etr__n_estimators": [2, 3, 5, 10, 50, 75],
            "etr__max_depth": [2, 3, 4, 5, 6, 7, 8, 9, 10],
            "etr__min_samples_leaf": [2, 3, 4, 5, 6, 7, 8, 9, 10]
        }
    },
    "Ada Boost Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                    ('ada', AdaBoostRegressor(DecisionTreeRegressor()))]),
        "param_grid": {
            "ada__n_estimators": [5, 10, 20, 50, 100, 125],
            "ada__learning_rate": [0.001, 0.01, 0.02, 0.04, 0.06],
        }
    },
    "XGBoost Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor),
                                    ('xgb', XGBRegressor())]),
        "param_grid": {
            "xgb__n_estimators": [5, 10, 20, 50, 100, 125],
            "xgb__learning_rate": [0.01, 0.1, 0.2],
            "xgb__max_depth": [2, 3, 5, 7]
        }
    }
}
```

```{python}
import matplotlib.pyplot as plt
import mlflow

from rice_yield.utils.paths import create_model_dir
from rice_yield.utils.plot_utils import validation_curve_display
from rice_yield.models.mlflow_utils import start_run, log_artifact, create_folder

# Iterate over candidate models
for model_name, model_data in candidate_models.items():
    pipe = model_data["pipeline"]
    param_grid = model_data["param_grid"]

    # Start an MLflow run under the appropriate experiment and run names
    with start_run(model_name=model_name, experiment_type="Validation", run_name="validation_curves") as run:

        # Get the folder to save plots and create it if missing
        folder_path = create_model_dir(model_name)
        # create_folder(str(folder_path))  # Ensure the directory exists

        # Loop over hyperparameters to plot validation curves
        for param, values in param_grid.items():
            fig, ax = validation_curve_display(
                estimator=pipe,
                X=X_train,
                y=y_train,
                param_name=param,
                param_range=values,
                cv=tscv,
                scoring='neg_root_mean_squared_error',
                negate_score=True
            )

            # Build the file path and save the figure
            param_name_safe = param.split('__')[-1]
            fig_path = folder_path / f"{param_name_safe}.png"
            fig.savefig(fig_path)
            plt.close(fig)

            # Log the saved plot as an artifact in MLflow
            log_artifact(str(fig_path))
            dataset = mlflow.data.from_pandas(df)
            mlflow.log_input(dataset)
mlflow.end_run()
```



```{python}
#| error: false
from rice_yield.utils.plot_utils import show_validation_curves

model, param, plot = show_validation_curves()
model
param
plot
```


# Chapter 8: Hyperparameter Optimization


## Dataset Registration

```{python}
from rice_yield.models.mlflow_utils import set_run_tags

dataset_path = get_data_file(get_data_dir("final"), "yield_processed.csv")

with start_run(model_name="", experiment_type="Dataset_Registration", run_name="Dataset_v1") as dataset_run:
    log_artifact(str(dataset_path))
    set_run_tags({"dataset_version": "v1"})
    dataset_run_id = dataset_run.info.run_id
mlflow.end_run()
```


```{python}
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
import numpy as np
import mlflow
from rice_yield.models.mlflow_utils import start_run, set_run_tags

def make_objective(model_name: str, model_class, param_fn):
    """
    Create an Optuna objective function for a specific model, using cross_val_score
    and logging everything with MLflow utilities.

    Args:
        model_name (str): Name of the model (e.g., "SVR").
        model_class: The model class (e.g., SVR, RandomForestRegressor).
        param_fn: Callable that takes a trial and returns a dictionary of hyperparameters.

    Returns:
        objective: A function suitable for Optuna's study.optimize().
    """
    def objective(trial):
        # Suggest hyperparameters
        params = param_fn(trial)

        # Start MLflow run for this trial
        with start_run(model_name=model_name, experiment_type="HPO", run_name=f"{model_name}_trial_{trial.number}", nest=True) as run:
            trial.set_user_attr("mlflow_run_id", mlflow.active_run().info.run_id)
            
            # Build pipeline
            model = Pipeline([
                ('preprocessor', preprocessor),  # your preprocessor defined in notebook
                (model_name.lower(), model_class(**params))
            ])

            # Cross-validate for RMSE
            rmse_scores = cross_val_score(
                model, X_train, y_train, 
                cv=tscv, scoring="neg_root_mean_squared_error"
            )
            rmse = -np.mean(rmse_scores)

            # Optional: log hyperparameters as tags
            set_run_tags(params)
            
            # Log RMSE metric
            mlflow.log_metric("rmse", rmse)

        return rmse

    return objective
```


```{python}
from sklearn.ensemble import RandomForestRegressor
# === Candidate models and their search spaces ===
search_spaces = {
    "SVR": {
        "model_class": SVR,
        "params": lambda trial: {
            "C": trial.suggest_float("C", 0.1, 100, log=True),
            "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
            "gamma": trial.suggest_float("gamma", 1e-4, 1e-1, log=True),
            # "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
        }
    },
    "RandomForest": {
        "model_class": RandomForestRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 50, 200),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 10)
        }
    },
    "XGB": {
        "model_class": XGBRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 50, 200),
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True)
        }
    }
}
```


```{python}
import rice_yield.models.optuna_utils as opt
import rice_yield.models.mlflow_utils as mlf
from rice_yield.utils.notebook_utils import save_model
from sklearn.model_selection import cross_validate
from rice_yield.utils.notebook_utils import save_model
from rice_yield.utils.paths import get_output_dir
```


```{python}
best_models = {}

for model_name, model_params in search_spaces.items():
    with mlf.start_run(model_name, "HPO", f"{model_name}_Optimization") as parent_run:
        study = opt.create_study(study_name=f"{model_name}_Study",
        direction="minimize",
        storage="sqlite:///../outputs/models_db.sqlite3")
        print("Optimization Started for", model_name)

        # opt.run_optimization(study,
        # objective_func=make_objective(model_name, model_params['model_class'], model_params['params']), n_trials=10)
        study.optimize(make_objective(model_name, model_params["model_class"], model_params["params"]), n_trials=10)

        print("Optimization completed for", model_name)

        best_trial = opt.get_best_trial(study)
        best_run_id = opt.get_best_run_id(study)
        mlf.set_run_tags({"best_trial_number": opt.get_best_trail_number(study)})
        mlf.log_params(opt.get_best_params(study))
        mlf.log_metrics("RMSE", best_trial.value)


        best_model = Pipeline([
            ('preprocessor', preprocessor),
            (model_name.lower(), model_params['model_class'](**best_trial.params))
        ])

        cv_results = cross_validate(
        best_model,
        X_train, 
        y_train,
        cv=tscv,
        scoring="neg_root_mean_squared_error",
        return_train_score=True)

        # Compute mean and std for train/test
        rmse_train = -cv_results["train_score"]
        # rmse_train = cv_results["train_score"]
        # rmse_test = cv_results["test_score"]
        rmse_test = -cv_results["test_score"]

        best_model.fit(X_train, y_train)

        output_folder = get_output_dir() / "ml_models"
        save_model(best_model, model_name, output_folder)

        # Save the best model to best_results
        best_models[model_name] = {
        "model": best_model,
        "best_params": opt.get_best_params(study),
        "study": study,
        "cv_results": {
            "train_score": rmse_train,
            # "train_std": rmse_train,
            # "test_mean": rmse_test,
            "test_score": rmse_test
        }
    }
    mlf.end_run()
```


```{python}
from shiny.express import render
from rice_yield.utils.plot_utils import plot_prediction_error

@render.ui
def dropdon_models():
    return ui.input_select(
        id="models",
        label="Select Model(s)",
        choices=sorted(best_models.keys()),  # Available models from the dictionary
    )

@render.plot
def prediction_error_plot():
    # req(input.model())  # Ensure a model is selected
    model_info = best_models[input.models()]  # Get model details
    model = model_info["model"]

    # Call the reusable plot function
    fig, ax = plot_prediction_error(y_train, model.predict(X_train), '')
    
    ax.set_title(f"Prediction Error for {input.models()}", fontsize=14, fontweight='bold')
    return fig

```


```{python}
from rice_yield.utils.plot_utils import plot_train_test_scores

@render.ui
def dropdwn_models():
    return ui.input_select(
        id="model_scores",
        label="Select Model(s)",
        choices=sorted(best_models.keys()),  # Available models from the dictionary
    )


@render.plot
def plot_train_test():
    # req(input.model_scores())
    model_name = input.model_scores()
    cv = best_models[model_name]["cv_results"]

    fig, ax = plot_train_test_scores(
        train_scores=cv['train_score'],
        test_scores=cv['test_score'],
        model_name=model_name,
        scoring_name="RMSE"
    )
    return fig


```