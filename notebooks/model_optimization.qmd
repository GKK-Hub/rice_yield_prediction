---
title: "Model Training and Optimization"
author: Gowtham Kumar K
date: September 25, 2025
link-external-newwindow: true
format:
  html:
    smooth-scroll: true
    encoding: utf-8
    toc: true
    toc-location: right
    toc-depth: 3
    toc-title: Chapters
    theme: zephyr
    # monofont: "JetBrains Mono"
    page-layout: full
    code-tools: true
    include-in-header: 
      text: |
        <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel="stylesheet">
        <style>
          code, pre {
            font-family: 'JetBrains Mono', monospace !important;
          }
        </style>
        <meta charset="UTF-8">
execute:
  enabled: true
  echo: true
  output: true
server: shiny
jupyter: python3

---

In this file, we\'re going to do model training and optimization. Finally, we\'ll save the best performing model and use that for inference and SHAP analysis.


```{python}
#| code-fold: true
import random
import warnings
import pandas as pd
import numpy as np

warnings.filterwarnings("ignore")
```


```{python}
#| code-fold: true
from rice_yield.utils.paths import get_data_file, get_data_dir
from rice_yield.utils.notebook_utils import render_table

file_path = get_data_file(get_data_dir("final"), "rice-yield.csv")

df = pd.read_csv(file_path)
```

---

## Chapter 6: Split Data - Train, Valid and Test

Let\'s sort the data before splitting the data.

```{python}
df = df.sort_values(['year', 'dist_name'], ignore_index=True)
render_table(df)
```

---

We\'ll use 80% of the data for training and the remaining for testing purpose.

---

```{python}
# cutoff = int(df['year'].nunique() * 0.8) + df['year'].min() 
# train_cutoff = int(df['year'].nunique() * 0.6) + df['year'].min() #
# train_df = df[df['year'] <= train_cutoff].copy()

# valid_cutoff = i
```

```{python}
# Experiment: train, valid and test split
years = sorted(df['year'].unique())
total_years = len(years)

# Calculate number of years for each split
train_years = int(total_years * 0.6)
val_years = int(total_years * 0.2)

train_cutoff = years[train_years - 1]  # Last year for training
val_cutoff = years[train_years + val_years - 1]  # 

train_df = df[df['year'] <= train_cutoff]
val_df = df[(df['year'] > train_cutoff) & (df['year'] <= val_cutoff)]
test_df = df[df['year'] > val_cutoff]
```

```{python}
# train_df = df[df['year'] <= cutoff].copy()
# test_df = df[df['year'] > cutoff].copy()
```

::: {.panel-tabset}
### Train DataFrame

```{python}
#| code-fold: true
render_table(train_df)
```

### Valid DataFrame

```{python}
render_table(val_df)
```

### Test DataFrame
```{python}
#| code-fold: true

render_table(test_df)
```
:::


```{python}
#| code-fold: true
# df['split'] = df['year'].apply(lambda x: "Train" if x <= cutoff else "Test")
```

```{python}
def assign_split(x, train_cutoff, val_cutoff):
    if x <= train_cutoff:
        return "Train"
    elif x <= val_cutoff:
        return "Val"
    else:
        return "Test"

df['split'] = df['year'].apply(lambda x: assign_split(x, train_cutoff, val_cutoff))
```

```{python}
from rice_yield.utils.plot_utils import train_test_plot
from shiny.express import ui, render, input


# plot_columns = [
#                 "temperature",
#                 "precipitation",
#                 "act_etranspiration",
#                 "pot_etranspiration",
#                 "yield",
#                 "water_deficit",
#                 "rainfall",
#                 "yield",
#                 "area"
#             ]

def show_splits():
    @render.ui()
    def drop_down_feature():
        choices = df.columns.difference(['split', 'dist_name', 'year']).tolist()
        default_choice = choices[0] if choices else None
        user_input = ui.input_select('feature', 
                                    label='Choose a feature',
                                    choices=choices,
                                    selected=default_choice)
        return user_input

    @render.plot()
    def plot_split():   
        fig, ax = train_test_plot(df, input.feature())
        return fig    
    return drop_down_feature, plot_split                    
```
```{python}
user_input, split_plot = show_splits()
user_input
split_plot
```

### Train, Valid and Test Set
```{python}
target = "yield"
X_train = train_df.drop(columns=target)
y_train = train_df[target]

X_val = val_df.drop(columns=target)
y_val = val_df[target]

X_test = test_df.drop(columns=target)
y_test = test_df[target]
```


### Time-series Split


```{python}
from sklearn import set_config
from sklearn.model_selection import TimeSeriesSplit

set_config(display="diagram")

tscv = TimeSeriesSplit(n_splits=5)
```

```{python}
#| code-fold: true
# for i, (train_idx, test_idx) in enumerate(tscv.split(X_train)):
#   print(f"Split {i + 1}: Train size = {len(train_idx)}, Test size = {len(test_idx)}")
```

## Chapter 7: Data Preprocessing

In this chapter, we\'re using a preprocessing pipleine to transform categoric data (`dist_name`) in our dataset. Also, some models like Linear Regression benefit from scaling numerical data.


```{python}
# Column Selector

from sklearn.compose import make_column_selector as selector

numeric_selector = selector(dtype_exclude=object)
categoric_selector = selector(dtype_include=object)
```


```{python}
numeric_columns = numeric_selector(X_train)
categoric_columns = categoric_selector(X_train)
```


### Preprocessor Pipeline
```{python}
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

categoric_transformer = OneHotEncoder(handle_unknown="ignore")
numeric_transformer = StandardScaler()

preprocessor = ColumnTransformer(transformers=[('categoric', categoric_transformer, categoric_columns),
('numeric', numeric_transformer, numeric_columns)], remainder="passthrough", verbose_feature_names_out=False)

preprocessor
```

# Chapter 7: Validation Curves

```{python}
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor
from xgboost import XGBRegressor

from sklearn.pipeline import Pipeline
```

```{python}
candidate_models = {
  "Ridge Regression": {
    "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('ridge', Ridge(max_iter=1000))]),
    "param_grid": {
        "ridge__alpha": [0.1, 1.0, 10.0, 100.0]  # Regularization strength
    }
},
    "Support Vector Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(max_iter=500))]),
        "param_grid": {
            "svr__C": [800, 1000, 1100],
            "svr__epsilon": [2, 2.5, 3],
            "svr__gamma": [0.00001, 0.0001, 0.001],
        }
    },

    "Extra Trees Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), ('etr', ExtraTreesRegressor())]),
        "param_grid": {
            "etr__min_samples_split": [2, 3, 5, 10, 15, 20],
            "etr__n_estimators": [2, 3, 5, 10, 50, 75],
            "etr__max_depth": [2, 3, 4, 5, 6, 7, 8, 9, 10],
            "etr__min_samples_leaf": [2, 3, 4, 5, 6, 7, 8, 9, 10]
        }
    },
    "Ada Boost Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor), 
                                    ('ada', AdaBoostRegressor(DecisionTreeRegressor()))]),
        "param_grid": {
            "ada__n_estimators": [5, 10, 20, 50, 100, 125],
            "ada__learning_rate": [0.001, 0.01, 0.02, 0.04, 0.06],
        }
    },
    "XGBoost Regressor": {
        "pipeline": Pipeline(steps=[('preprocessor', preprocessor),
                                    ('xgb', XGBRegressor())]),
        "param_grid": {
            "xgb__n_estimators": [5, 10, 20, 50, 100, 125],
            "xgb__learning_rate": [0.01, 0.1, 0.2],
            "xgb__max_depth": [2, 3, 5, 7]
        }
    }
}
```

```{python}
# import matplotlib.pyplot as plt
# import mlflow

# from rice_yield.utils.paths import create_model_dir
# from rice_yield.utils.plot_utils import validation_curve_display
# from rice_yield.models.mlflow_utils import start_run, log_artifact, create_folder

# Iterate over candidate models
# for model_name, model_data in candidate_models.items():
#     pipe = model_data["pipeline"]
#     param_grid = model_data["param_grid"]

#     # Start an MLflow run under the appropriate experiment and run names
#     with start_run(model_name=model_name, experiment_type="Validation", run_name="validation_curves") as run:

#         # Get the folder to save plots and create it if missing
#         folder_path = create_model_dir(model_name)
#         # create_folder(str(folder_path))  # Ensure the directory exists

#         # Loop over hyperparameters to plot validation curves
#         for param, values in param_grid.items():
#             fig, ax = validation_curve_display(
#                 estimator=pipe,
#                 X=X_train,
#                 y=y_train,
#                 param_name=param,
#                 param_range=values,
#                 cv=tscv,
#                 scoring='neg_root_mean_squared_error',
#                 negate_score=True
#             )

#             # Build the file path and save the figure
#             param_name_safe = param.split('__')[-1]
#             fig_path = folder_path / f"{param_name_safe}.png"
#             fig.savefig(fig_path)
#             plt.close(fig)

#             # Log the saved plot as an artifact in MLflow
#             log_artifact(str(fig_path))
#             dataset = mlflow.data.from_pandas(df)
#             mlflow.log_input(dataset)
# mlflow.end_run()
```



```{python}
# #| error: false
# from rice_yield.utils.plot_utils import show_validation_curves

# model, param, plot = show_validation_curves()
# model
# param
# plot
```


# Chapter 7a: Validation Curves Experiment

```{python}
model_names = list(candidate_models.keys())
default_model = model_names[0] if model_names else None
```

```{python}
# from rice_yield.utils.plot_utils import validation_curve_display
# from shiny.express import ui, render, input

# def show_validation_curves():
#     @render.ui()
#     def dropdown_model():
#         return ui.input_select(id='model',
#                                 label='Select Model',
#                                 choices=model_names,
#                                 selected=default_model)

#     @render.ui
#     def dropdown_param():
#         params =  list(candidate_models[input.model()]['param_grid'].keys())
#         default_param = params[0] if params else None
#         return ui.input_select(id='param',
#                                 label='Select Hyperparameter',
#                                 choices=params,
#                                 selected=default_param)

#     @render.plot(width=800, height=500)  # type: ignore
#     def show_plot():
#         plot_params = {
#                 'estimator': candidate_models[input.model()]['pipeline'],
#                 'X': X_train,
#                 'y': y_train,
#                 'param_name': input.param(),
#                 'param_range': candidate_models[input.model()]['param_grid'][input.param()],
#                 'cv': tscv,
#                 'scoring': 'neg_root_mean_squared_error',
#             }
#         fig, ax = validation_curve_display(**plot_params)
#         return fig
#     return dropdown_model, dropdown_param, show_plot

```


```{python}
from rice_yield.utils.plot_utils import validation_curve_display
from shiny.express import ui, render, input

def show_validation_curves():
    # Create reactive value to track when parameters are properly updated
    # params_updated = reactive.Value(True)
    @render.ui()
    def dropdown_model():
        return ui.input_select(id='model',
                                label='Select Model',
                                choices=model_names,
                                selected=default_model)
    
    @render.ui()
    def dropdown_param():
        # Signal that parameters are being updated
        # params_updated.set(False)
        
        # Get parameters for the selected model
        params = list(candidate_models[input.model()]['param_grid'].keys())
        default_param = params[0] if params else None
        
        # Signal that parameters are now updated
        # params_updated.set(True)
        
        return ui.input_select(id='param',
                                label='Select Hyperparameter',
                                choices=params,
                                selected=default_param)
    
    @render.plot(width=900, height=500)  # type: ignore
    def show_plot():
        # Wait for parameters to be properly updated
        # if not params_updated.get():
        #     return None
            
        # Add defensive checks to prevent race conditions
        try:
            model_name = input.model()
            param_name = input.param()
            
            # Check if the model exists
            if model_name not in candidate_models:
                return None
            
            # Check if the parameter exists for the selected model
            available_params = list(candidate_models[model_name]['param_grid'].keys())
            if param_name not in available_params:
                # If the current param doesn't exist for this model, use the first available param
                param_name = available_params[0] if available_params else None
                
            # if param_name is None:
            #     return None
            
            plot_params = {
                'estimator': candidate_models[model_name]['pipeline'],
                'X': X_train,
                'y': y_train,
                'param_name': param_name,
                'param_range': candidate_models[model_name]['param_grid'][param_name],
                'cv': tscv,
                'scoring': 'neg_root_mean_squared_error',
            }
            
            fig, ax = validation_curve_display(**plot_params)
            return fig
            
        except (KeyError, TypeError, ValueError) as e:
            # Return None or a placeholder plot during transitions
            return None
    
    return dropdown_model, dropdown_param, show_plot
```

```{python}
#| error: false
model, param, plot = show_validation_curves()
model
param
plot
```
# Chapter 8: Hyperparameter Optimization


```{python}
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
import numpy as np
import mlflow
from rice_yield.models.mlflow_utils import start_run, set_run_tags

def make_objective(model_name: str, model_class, param_fn):
    """
    Create an Optuna objective function for a specific model, using cross_val_score
    and logging everything with MLflow utilities.

    Args:
        model_name (str): Name of the model (e.g., "SVR").
        model_class: The model class (e.g., SVR, RandomForestRegressor).
        param_fn: Callable that takes a trial and returns a dictionary of hyperparameters.

    Returns:
        objective: A function suitable for Optuna's study.optimize().
    """
    def objective(trial):
        # Suggest hyperparameters
        params = param_fn(trial)

        # Start MLflow run for this trial
        with start_run(model_name=model_name, experiment_type="HPO", run_name=f"{model_name}_trial_{trial.number}", nest=True) as run:
            trial.set_user_attr("mlflow_run_id", mlflow.active_run().info.run_id)
            
            # Build pipeline
            model = Pipeline([
                ('preprocessor', preprocessor),  # your preprocessor defined in notebook
                (model_name.lower(), model_class(**params))
            ])

            # Cross-validate for RMSE
            rmse_scores = cross_val_score(
                model, X_train, y_train, 
                cv=tscv, scoring="neg_root_mean_squared_error"
            )
            rmse = -np.mean(rmse_scores)

            # Optional: log hyperparameters as tags
            set_run_tags(params)
            
            # Log RMSE metric
            mlflow.log_metric("rmse", rmse)

        return rmse

    return objective
```


```{python}
from sklearn.ensemble import RandomForestRegressor
# === Candidate models and their search spaces ===
search_spaces = {
    "SVR": {
        "model_class": SVR,
        "params": lambda trial: {
            "C": trial.suggest_float("C", 0.1, 100, log=True),
            "epsilon": trial.suggest_float("epsilon", 0.01, 1.0, log=True),
            "gamma": trial.suggest_float("gamma", 1e-4, 1e-1, log=True),
            # "kernel": trial.suggest_categorical("kernel", ["rbf", "poly", "linear"])
        }
    },
    "RandomForest": {
        "model_class": RandomForestRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 50, 200),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 10)
        }
    },
    "XGB": {
        "model_class": XGBRegressor,
        "params": lambda trial: {
            "n_estimators": trial.suggest_int("n_estimators", 50, 200),
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True)
        }
    }
}
```


```{python}
import rice_yield.models.optuna_utils as opt
import rice_yield.models.mlflow_utils as mlf
from rice_yield.utils.notebook_utils import save_model
from sklearn.model_selection import cross_validate
from rice_yield.utils.notebook_utils import save_model
from rice_yield.utils.paths import get_output_dir
```


```{python}
from sklearn.model_selection import cross_val_score
model_results = {}

for model_name, model_params in search_spaces.items():
    with mlf.start_run(model_name, "HPO", f"{model_name}_Optimization") as parent_run:
        study = opt.create_study(study_name=f"{model_name}_Study",
        direction="minimize",
        storage="sqlite:///../outputs/models_db.sqlite3")
        print("Optimization Started for", model_name)

        # opt.run_optimization(study,
        # objective_func=make_objective(model_name, model_params['model_class'], model_params['params']), n_trials=10)
        study.optimize(make_objective(model_name, model_params["model_class"], model_params["params"]), n_trials=10)

        print("Optimization completed for", model_name)

        best_trial = opt.get_best_trial(study)
        best_run_id = opt.get_best_run_id(study)
        mlf.set_run_tags({"best_trial_number": opt.get_best_trail_number(study)})
        mlf.log_params(opt.get_best_params(study))
        mlf.log_metrics("RMSE", best_trial.value)


        best_model = Pipeline([
            ('preprocessor', preprocessor),
            (model_name.lower(), model_params['model_class'](**best_trial.params))
        ])

        # cv_results = cross_validate(
        # best_model,
        # X_train, 
        # y_train,
        # cv=tscv,
        # scoring="neg_root_mean_squared_error",
        # return_train_score=True)

        # # Compute mean and std for train/test
        # rmse_train = -cv_results["train_score"]
        # # rmse_train = cv_results["train_score"]
        # # rmse_test = cv_results["test_score"]
        # rmse_test = -cv_results["test_score"]

        best_model.fit(X_train, y_train)

        train_scores = cross_val_score(best_model, X_train, y_train, cv=tscv, scoring='neg_root_mean_squared_error')

        output_folder = get_output_dir() / "trained_models"
        save_model(best_model, model_name, output_folder)

        # Save the best model to best_results
        model_results[model_name] = {
        "model": best_model,
        "best_params": opt.get_best_params(study),
        "study": study,
        "cv_results": {
            "train_score": train_scores,
            # "test_score": rmse_test
        }
    }
    mlf.end_run()
```


```{python}
from shiny.express import render
from rice_yield.utils.plot_utils import plot_prediction_error

@render.ui
def dropdon_models():
    return ui.input_select(
        id="models",
        label="Select Model(s)",
        choices=sorted(model_results.keys()),  # Available models from the dictionary
    )

@render.plot
def prediction_error_plot():
    # req(input.model())  # Ensure a model is selected
    model_info = model_results[input.models()]  # Get model details
    model = model_info["model"]

    # Call the reusable plot function
    fig, ax = plot_prediction_error(y_val, model.predict(X_val))
    
    fig.suptitle(f"Validation Set Error for {input.models()}", fontsize=14, fontweight='bold', fontfamily='Arial')
    return fig

```


```{python}
from rice_yield.utils.plot_utils import plot_train_test_scores
from shiny.express import ui, render, input

@render.ui
def dropdwn_models():
    return ui.input_select(
        id="model_scores",
        label="Select Model(s)",
        choices=sorted(model_results.keys()),  # Available models from the dictionary
    )


@render.plot
def plot_train_test():
    model_name = input.model_scores()
    cv = model_results[model_name]["cv_results"]
    val_scores = cross_val_score(best_model, X_val, y_val, cv=tscv, scoring='neg_root_mean_squared_error')

    fig, ax = plot_train_test_scores(
        train_scores=-cv['train_score'],
        test_scores=-val_scores,
        model_name=model_name,
        scoring_name="RMSE"
    )
    return fig
```


```{python}
data_dir = get_data_dir("final")
save_model(model_results['SVR']['model'], 'svr', get_output_dir()/"best_model")
X_train.to_csv(data_dir/"X_train.csv", index=False)
X_test.to_csv(data_dir/"X_test.csv", index=False)
X_val.to_csv(data_dir/"X_val.csv", index=False)
y_train.to_csv(data_dir/"y_train.csv", index=False)
y_val.to_csv(data_dir/"y_val.csv", index=False)
y_test.to_csv(data_dir/"y_test.csv", index=False)
```